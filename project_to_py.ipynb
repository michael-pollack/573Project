{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14ad144",
   "metadata": {},
   "source": [
    "Welcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements til I figure out the other way \n",
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_train = pd.read_parquet('data/Elife/train-00000-of-00001.parquet')\n",
    "df_elife_test = pd.read_parquet('data/Elife/test-00000-of-00001.parquet')\n",
    "df_elife_validation = pd.read_parquet('data/Elife/validation-00000-of-00001.parquet')\n",
    "df_elife_validation.head(5)\n",
    "\n",
    "df_elife = df_elife_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plos_train_1 = pd.read_parquet('data/PLOS/train-00000-of-00003.parquet')\n",
    "# df_plos_train_2 = pd.read_parquet('data/PLOS/train-00001-of-00003.parquet')\n",
    "# df_plos_train_3 = pd.read_parquet('data/PLOS/train-00002-of-00003.parquet')\n",
    "# df_plos_test = pd.read_parquet('data/PLOS/test-00000-of-00001.parquet')\n",
    "# df_plos_validation = pd.read_parquet('data/PLOS/validation-00000-of-00001.parquet')\n",
    "# df_plos_train_3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37571c87-80d4-4917-bdb3-b41a953a78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45cd62-790b-498e-936f-700e205bfbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(doc):\n",
    "    \"\"\"A function to strip punctuation, strip stopwords, casefold, lemmatize,\n",
    "    And part of speech tag words for clean data for modeling\"\"\"\n",
    "    custom_stops = ['doi', 'figure', 'elife', 'et', 'al']\n",
    "    sw = stopwords.words('english')\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw and word not in custom_stops]\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c35da-eb1a-450b-9645-8a9b2b64a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listifies contents in article column\n",
    "articles_list = df_elife.article.tolist()\n",
    "print(len(articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf24d2-5a82-4380-8ce5-c546ef5616bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize all words in all articles\n",
    "article_tokens = []\n",
    "article_tokens.extend(word_tokenize(i) for i in articles_list)\n",
    "print(len(article_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0022e6f-a077-4ca9-bcfe-404ff756dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes raw article data into single list of tokens, not broken into sentences\n",
    "article_tokens_flat = [word for doc in article_tokens for word in doc]\n",
    "\n",
    "print(article_tokens_flat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39f00f-dc21-4dde-9611-c9da883192c0",
   "metadata": {},
   "source": [
    "# Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a50e6-ff10-49fd-a8b1-f704f4ac09b9",
   "metadata": {},
   "source": [
    "The first step to our data praparation was to remove large outliers. If you look at the Elife Document Length bar graph above that at about 110000 words in length we have a handful of very lengthy documents. This will allow us to cut back on the amount of processing and time. We will also benefit from not having content from extra large documents over weighting our tf-idf scores. We removed 53 large outlier articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39aae1-ed56-46d7-8ee1-29a26d64bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to remove documents of more than 11000 words\n",
    "#df_elife_train = df_elife_train[df_elife_train['article'].str.len() < 110000 ]\n",
    "print(df_elife.shape)\n",
    "articles_no_outliers = df_elife.article.tolist()\n",
    "print(len(articles_no_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde823a-9d5c-42f1-bcb5-c464d512e19d",
   "metadata": {},
   "source": [
    "One of the other problems we encountered when inspecting the data is the large amount of academic in text citations. These citations cause several issues, they're so common that they occur far more frequently than content words do, which creates extra noise. They're also full of punctuation which messes with the mechanics of the sentence tokenizer we selected. This resulted in a lot of \"false\" sentences. To solve these issues we removed the citations and moved the function to do so up above the more standard cleaning function. This allows us to keep our sentence index stable throughout the process while also cutting the amount of sentences per doc by almost half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0390e-5b9c-423f-9e63-52a93dc9d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initally removes things between parenthesis to keep sentence number stable in later process\n",
    "def remove_between_parens(doc):\n",
    "    doc = re.sub(r\"\\([^()]*\\)|\\[[^\\]]*\\]|\\{[^}]*\\}\", \"\", doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbb3dd-bc51-4786-8625-64dca32998f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run data through data parens cleaning function\n",
    "\n",
    "no_parens_corpus = []\n",
    "for doc in articles_no_outliers:\n",
    "    no_parens_corpus.append(remove_between_parens(doc))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_parens_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422da89c-1b10-45fc-82f8-5dca952c23e9",
   "metadata": {},
   "source": [
    "To run this code on new data you will need to uncomment and run the following two cells. To recreate the exact results from our interpretation you will need to use the df_elife_train_clean.csv whcih can be located in our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63af546-2de3-4634-86e7-5e7a2e7255c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #runs data through data cleaning function\n",
    "# run this cell unless otehrwise specified\n",
    "\n",
    "clean_corpus = []\n",
    "for doc in no_parens_corpus:\n",
    "    clean_corpus.append(data_cleaner(doc))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3b1dc-929b-423a-a94e-48e11aeca8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell unless otehrwise specified\n",
    "\n",
    "df_elife['clean'] = clean_corpus\n",
    "\n",
    "df_elife.head()\n",
    "df_elife.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1854af-e835-4972-9523-64025c1768ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife.to_csv('df_elife_clean.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb79893",
   "metadata": {},
   "source": [
    "WORK ON THIS CELL!!\n",
    "WHY DOES TRAIN_CLEAN HAVE 4000+ LINES AND TRAIN HAS THE CORRECT 142?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d160667-d8e2-45e5-b525-79c1734e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if you have a copy of the cleaned training data\n",
    "df_elife_clean = pd.read_csv('df_elife_clean.csv')\n",
    "# df_elife_train_clean.head()\n",
    "df_elife_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f640cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_clean.info()\n",
    "df_elife.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bb19f-9854-4070-ba8f-b07328f18f3e",
   "metadata": {},
   "source": [
    "Below are a word cloud and a frequency distribution. They show some of the more frequent words found within our data. This kind of data analysis was helpful in identifying important features in the data and looking at what we might need to strip out that isn't caught in the standard stoplist wordlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c5c06-c6ec-4810-a156-ad9b3d1d7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wordcloud_maker(df, stopwords = None):\n",
    "#     \"\"\"cretes words clouds from cleaned data\"\"\"\n",
    "#     all_clean = \" \".join(review for review in df.clean)\n",
    "#     wordcloud = WordCloud(stopwords = stopwords, background_color=\"white\").generate(all_clean)\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0772c-33ea-4e09-aa35-433fa4856739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud_maker(df_elife_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05f63b-939e-452e-b067-3da2d2517cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking custom stop lists and cleaning worked. Cut down 246k words\n",
    "clean_doc_list = df_elife_clean.clean.tolist()\n",
    "freqdist_maker(clean_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7afa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5476a0b-9f4e-4351-977c-6d8c585ca9b6",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a0113",
   "metadata": {},
   "source": [
    "To begin processing our data to create extractive summaries we implement a method of assigning vectors to words called Term Frequency - Inverse Document Frequency(tf-idf). This process allows us to assign a concept of importance and meaning to words in our documents represented by a number. This is done by first getting the term frequency, a count of each term in its document divided by the total number of terms in that document. Then the inverse document frequency, which is the total number of documents in our corpus divided by the number of documents containing some specific word, that value is then multiplied by log base e. We will do this for each document to produce a unique set of vector word pairs. The higher a vector, the more important that word is to that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c99be3-20e5-483c-840f-d257e4bd0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', ngram_range=(1,1))\n",
    "\n",
    "# Get the clean column from the elife dataframe\n",
    "elife_clean = df_elife_clean.clean.tolist()\n",
    "\n",
    "# Calculate the TF-IDF score for unigrams and bigrams using the clean data\n",
    "elife_t_c = vectorizer.fit_transform(elife_clean)\n",
    "\n",
    "#print(elife_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff34ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elife_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ab009",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elife_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for data\n",
    "vocab = list(vectorizer.vocabulary_.items())\n",
    "print(vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ad1c3-4587-4c60-aac0-0c557dc7fc3b",
   "metadata": {},
   "source": [
    "# Extractive Summary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5bcc1-e8da-4af2-b65e-0b54e6dff85e",
   "metadata": {},
   "source": [
    "Once we have all of our vectors its time to start assembling our summaries. Summaries are computed by assigning every word in our data a vector and then summing those vectors up for each sentence so that each sentence in a document recieves a score. Sentences with higher scores or deemed more informative and therefore more important to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28152d-2b66-46ab-94c7-3b86070c0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_value_creator_2(doc_sents: list, vect_obj, doc_index) -> list: # List of sentences in a document -> list of tuples (index, sum)\n",
    "    \"\"\" \n",
    "    A function to take in a single article at a time, split the article by sentences, clean those sentences, split each sentence by words,\n",
    "    match each word with its vector, sum the vectors and returns a list of tuples (sentence index, vector sums)\n",
    "        \"\"\"\n",
    "    sent_index_val_dict = [] # Stores tuples (sentence_index, sum)\n",
    "\n",
    "    for i, sent in enumerate(doc_sents): # For every index and sentence in doc_sents\n",
    "\n",
    "        clean_sent = data_cleaner(sent) # Clean the sentence\n",
    "        c = 0.0 # The vector sum (starting at 0)\n",
    "        sent_split = clean_sent.split() # Split sentence by words\n",
    "\n",
    "        for word in sent_split: # For every word in the sentence\n",
    "            if word in vectorizer.vocabulary_: # If the word is in the TF-IDF vocabulary\n",
    "                vec_val = vect_obj[doc_index, vectorizer.vocabulary_[word]] # Get the vector score from TF_IDF vectorizer\n",
    "                c += vec_val # Add the vector score to the total sentence score\n",
    "        sent_index_val_dict.append((i,float(c))) # Append a tuple of (index, score) for the sentence \n",
    "\n",
    "    return sent_index_val_dict # Return the list of sentence (index, score) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd1b59",
   "metadata": {},
   "source": [
    "The following cell executes the main TF-IDF extractive summarization and saves the summaries as a new column in the df_elife_train_clean dataframe called 'tfidf_summary'. The summary_percent variable is an adjustable parameter that determines the length of the extractive summaries represented as a percentage of the original document's sentences. \n",
    "\n",
    "The overall process for one iteration of the for-loop (which loops through all the documents):\n",
    "1) The document is sent to the method remove_between_parens to have citations and other information enclosed in parenthese removed.\n",
    "2) The document is split into a list of sentences (doc_sents).\n",
    "3) The extranctive summary length (number of sentences) is calculated by summary_percent*doc_length. \n",
    "4) Each sentence in the document is given a score as calculated by the sentence_value_creator_2 function. These scores are saved in a list consisting of tuples of (sentence_index, sentence_score). The sentence_index is the index of this sentence in the original doc_sents.\n",
    "5) The list of tuples is sorted by score and then cropped to contain only the highest scoring top_num sentences. \n",
    "6) The list of tuples is then sorted by index so that the final summary can consist of sentences in their original order.\n",
    "5) Finally, the sentences represented by the top-scoring indices are concatenated into a string, in order, and that string is added to a list of summaries as the summary for the current document.\n",
    "\n",
    "After the loop, the list of extractive TF-IDF summaries is added as a new column in the elife training dataframe called 'tfidf_summary'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2d933-52dd-4d35-b84b-3cf651c90b93",
   "metadata": {},
   "source": [
    "Once every sentence in every article has been given a score we rank them from highest to lowest, take the top 60% of sentences in proportion the original number of sentences in the article and sort them based on order of appearence in accordance to their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdf719",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_percent = 0.4 # Percentage of total document sentences to save as the summary\n",
    "doc_summaries = [] # List to store the document summaries\n",
    "doc_list = df_elife_clean.article.tolist() # List of all the docs in elife train\n",
    "\n",
    "df_elife_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for doc_index, doc in enumerate(doc_list): # For every document\n",
    "    no_parens = remove_between_parens(doc) # Remove citations and other parentheses from the document\n",
    "    doc_sents = nltk.sent_tokenize(no_parens) # Split the document into sentences\n",
    "    doc_length = len(doc_sents) # Get the total number of sentences in the document\n",
    "    top_num = int(summary_percent * doc_length) # Calculate the number of sentences to keep for the summary\n",
    "    sent_scores = sentence_value_creator_2(doc_sents, elife_t_c, doc_index) # Get list of (index, score) pairs for all the document sentences\n",
    "    sorted_scores = sorted(sent_scores, key=lambda x: x[1]) # Sort based on second tuple object; sort by score\n",
    "    sorted_scores = sorted_scores[-top_num:] # Crop to just the top top_num sents\n",
    "    sorted_sents = sorted(sorted_scores, key = lambda x: x[0]) # Sort the top sents by index so they are in the logical order\n",
    "    doc_summary = \"\" # Save summary of the document as string\n",
    "    for (index, score) in sorted_sents: # For every (index, score) pair\n",
    "        sent = doc_sents[index] # Get the original sentence using the index\n",
    "        doc_summary = doc_summary + sent + \" \" # Add the original sent to the full summary \n",
    "    doc_summaries.append(doc_summary) # Add the summary to the list of all summaries\n",
    "\n",
    "# Save the summaries as a new column in the elife train dataframe\n",
    "df_elife_clean['tfidf_summary'] = doc_summaries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d693f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(doc_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the summaries to a csv file\n",
    "df_elife_clean.to_csv('elife_summaries.csv', index=True)\n",
    "\n",
    "# save the tfidf_summary column to a text file\n",
    "with open('elife_summaries.txt', 'w') as f:\n",
    "    for summary in df_elife_clean['tfidf_summary']:\n",
    "        f.write(summary + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4e85d-2569-4bd3-a85e-6fad6373b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = df_elife_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef351a-a062-4de0-a060-c75f26f6c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15005880-185b-4ee2-83e8-14057b2953fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_top_ten = top_10.tfidf_summary.tolist()\n",
    "orig_sum_top_ten = top_10.summary.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6d156-a706-4b24-9ede-43cd82328412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_top_ten[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29d9e4-5002-4b27-aa3c-165226b28d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_sum_top_ten[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1866c1-8c60-4d3d-8ded-04853091a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = df_elife_clean.tfidf_summary.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b713668-2dab-4258-a1c9-4eea44ff68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4b478-2641-4a78-b538-01a92d6da295",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = 0\n",
    "for i in summary_list:\n",
    "    for j in i:\n",
    "        if '\\n' in j:\n",
    "            #print(j)\n",
    "            catch+=1\n",
    "print(catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57592e42-ccd8-4d48-83e5-1ceef9f1aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elife_summaries.txt', 'w') as f:\n",
    "    for summ in summary_list:\n",
    "        f.write(summ.replace('\\n','') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db982c9-a643-4ec3-920e-7a0b87194605",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elife_summaries.txt', 'r') as g:\n",
    "    lines = g.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01dcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_elife_test to a text file\n",
    "with open('elife_test_reference.txt', 'w') as f:\n",
    "    for article in df_elife_test.article:\n",
    "        f.write(article + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 100 lines of the df_elife_clean.tfidf_summary column and save to a text file\n",
    "with open('elife_summaries100.txt', 'w') as f:\n",
    "    for summary in df_elife_clean.tfidf_summary[:100]:\n",
    "        f.write(summary + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 100 lines of the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "573",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
