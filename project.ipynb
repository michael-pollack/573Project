{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14ad144",
   "metadata": {},
   "source": [
    "Welcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "#pip installs a list of libraries\n",
    "#list can be found in requirements.txt\n",
    "\n",
    "#important import names\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements til I figure out the other way \n",
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "import json\n",
    "\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('.')\n",
    "#from importables import *\n",
    "#grabs all imports from import.py\n",
    "\n",
    "#This is not working but it should be >>>:(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_train = pd.read_parquet('data/Elife/train-00000-of-00001.parquet')\n",
    "df_elife_test = pd.read_parquet('data/Elife/test-00000-of-00001.parquet')\n",
    "df_elife_validation = pd.read_parquet('data/Elife/validation-00000-of-00001.parquet')\n",
    "df_elife_validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plos_train_1 = pd.read_parquet('data/PLOS/train-00000-of-00003.parquet')\n",
    "df_plos_train_2 = pd.read_parquet('data/PLOS/train-00001-of-00003.parquet')\n",
    "df_plos_train_3 = pd.read_parquet('data/PLOS/train-00002-of-00003.parquet')\n",
    "df_plos_test = pd.read_parquet('data/PLOS/test-00000-of-00001.parquet')\n",
    "df_plos_validation = pd.read_parquet('data/PLOS/validation-00000-of-00001.parquet')\n",
    "df_plos_train_3.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37571c87-80d4-4917-bdb3-b41a953a78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45cd62-790b-498e-936f-700e205bfbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(doc):\n",
    "    \"\"\"A function to strip punctuation, strip stopwords, casefold, lemmatize,\n",
    "    And part of speech tag words for clean data for modeling\"\"\"\n",
    "    \n",
    "    sw = stopwords.words('english')\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    #print(doc)\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    #print(doc)\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    #print(' '.join(doc))\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c35da-eb1a-450b-9645-8a9b2b64a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listifies contents in articl column\n",
    "articles_list = df_elife_train.article.tolist()\n",
    "print(len(articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509de2b3-ec02-40cb-8eb4-64608f79234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf24d2-5a82-4380-8ce5-c546ef5616bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize all words in all articles\n",
    "article_tokens = []\n",
    "for i in articles_list:\n",
    "    article_tokens.append(word_tokenize(i))\n",
    "print(article_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0022e6f-a077-4ca9-bcfe-404ff756dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes raw article data into single list of tokens, not broken into sentences\n",
    "article_tokens_flat = [word for doc in article_tokens for word in doc]\n",
    "\n",
    "print(article_tokens_flat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74d943-2d70-47ab-83d5-3ff88810dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect raw tokens\n",
    "all_words_raw = []\n",
    "\n",
    "for w in article_tokens_flat :\n",
    "    all_words_raw.append(w)\n",
    "    \n",
    "print(len(all_words_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a78bbf-52a0-44f8-84a3-d38e41c55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets count of tokens and types\n",
    "tokens = all_words_raw\n",
    "types = set(tokens)\n",
    "len(types), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c544cbf-1532-486e-bde0-33ec974e3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = nltk.FreqDist(tokens)\n",
    "fdist1\n",
    "#create frequency distribution, checks all tokens and how often they occur in the vocab\n",
    "#prints out top 10 most used, these are usally stopwrods as the data has not been cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6996257-ae0b-4400-9b54-195ad6ee4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1.plot(25, title = 'Raw Freqeuncy Distribution', cumulative = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d16b7-f8d0-4149-98fd-f78ea9fbf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1.hapaxes()[:10]\n",
    "#first ten unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef77f0-2d5a-498d-ac95-592bc8428eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "elife_doc_lengths = []\n",
    "\n",
    "for doc in articles_list:\n",
    "    elife_doc_lengths.append(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202cbb1-1b82-41eb-9de5-c3bb0c34da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 20\n",
    "fig, axs = plt.subplots(1, sharey=True, tight_layout=True)\n",
    "axs.hist(elife_doc_lengths, bins=n_bins)\n",
    "axs.set_title(\"Elife Document Length\");\n",
    "axs.set_xlabel('Numer of Words per Document');\n",
    "axs.set_ylabel(\"Document Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63af546-2de3-4634-86e7-5e7a2e7255c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run data through data cleaning function\n",
    "\n",
    "clean_corpus = []\n",
    "for doc in articles_list:\n",
    "    clean_corpus.append(data_cleaner(doc))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3b1dc-929b-423a-a94e-48e11aeca8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elife_train['clean'] = clean_corpus\n",
    "\n",
    "df_elife_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c5c06-c6ec-4810-a156-ad9b3d1d7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_maker(df, stopwords = None):\n",
    "    \"\"\"cretes words clouds from cleaned data\"\"\"\n",
    "    all_clean = \" \".join(review for review in df.clean)\n",
    "    wordcloud = WordCloud(stopwords = stopwords, background_color=\"white\").generate(all_clean)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0772c-33ea-4e09-aa35-433fa4856739",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_maker(df_elife_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05f63b-939e-452e-b067-3da2d2517cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These results are extremely funny. We might want to create a custom stopwords \n",
    "#list to remove things like et al, figure, doi, dx, org, elife etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
